{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aKiAE8s2WQM",
        "outputId": "8981f778-9ac4-4323-d85e-0f83ba6ed6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_dir = '/content/drive/MyDrive/10 DMC/X-Ray'\n",
        "# Check if the path exists\n",
        "if os.path.exists(dataset_dir):\n",
        "    print(\"Dataset directory exists.\")\n",
        "    print(\"Contents of dataset_dir:\", os.listdir(dataset_dir))\n",
        "    print(\"Contents of train folder:\", os.listdir(os.path.join(dataset_dir, 'train')))\n",
        "else:\n",
        "    print(\"Dataset directory does NOT exist. Check the path.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSbXzOtVQ4B6",
        "outputId": "7dce36cc-7011-4d49-a7f7-17d6541e1e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory does NOT exist. Check the path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import os\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/10 DMC/X-Ray'\n",
        "\n",
        "# Image size and batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Only rescale for testing\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=dataset_dir + '/train',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load testing data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=dataset_dir + '/test',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Verify dataset loading\n",
        "print(\"Classes in training data:\", train_generator.class_indices)\n",
        "print(\"Number of training samples:\", train_generator.samples)\n",
        "print(\"Number of testing samples:\", test_generator.samples)\n",
        "'''\n",
        "# Data Preprocessing and Dataset Loading\n",
        "# Assuming you have a dataset with images stored in directories like 'COVID19', 'NORMAL', 'PNEUMONIA'\n",
        "dataset_dir =  '/content/drive/MyDrive/10 DMC/X-RAY'  # Replace 'Dataset' with the actual folder name in your Drive\n",
        " # Change this to your dataset path\n",
        "img_size = (224, 224)  # Image size for model input\n",
        "\n",
        "# Use ImageDataGenerator for image loading and augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(os.path.join(dataset_dir, 'train'), target_size=img_size, batch_size=32, class_mode='categorical')\n",
        "val_data = test_datagen.flow_from_directory(os.path.join(dataset_dir, 'val'), target_size=img_size, batch_size=32, class_mode='categorical')\n",
        "test_data = test_datagen.flow_from_directory(os.path.join(dataset_dir, 'test'), target_size=img_size, batch_size=32, class_mode='categorical')\n",
        "\n",
        "'''\n",
        "\n",
        "# Model Definitions (4 models as examples)\n",
        "def Create_model(input_size=(224, 224, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_size),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(3, activation='softmax')  # Change '3' to the number of classes you have\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def Create_model_2(input_size=(224, 224, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3, 3), activation='relu', input_shape=input_size),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(3, activation='softmax')  # Change '3' to the number of classes you have\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def Create_model_3(input_size=(224, 224, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(128, (3, 3), activation='relu', input_shape=input_size),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(3, activation='softmax')  # Change '3' to the number of classes you have\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def Create_model_4(input_size=(224, 224, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(256, (3, 3), activation='relu', input_shape=input_size),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(512, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(3, activation='softmax')  # Change '3' to the number of classes you have\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the models\n",
        "def model_compiling(model, loss='categorical_crossentropy', optimizer='adam'):\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Train models and save their weights\n",
        "model1 = Create_model()\n",
        "model_compiling(model1)\n",
        "model2 = Create_model_2()\n",
        "model_compiling(model2)\n",
        "model3 = Create_model_3()\n",
        "model_compiling(model3)\n",
        "model4 = Create_model_4()\n",
        "model_compiling(model4)\n",
        "\n",
        "# Define callbacks\n",
        "model_path_1 = \"Model_1.h5\"\n",
        "checkpoint_1 = ModelCheckpoint(filepath=model_path_1, save_weights_only=True, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystop = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=6, factor=0.2, verbose=1, min_lr=0.00000001)\n",
        "\n",
        "# Train model1 and save weights\n",
        "history1 = model1.fit(train_data, validation_data=val_data, epochs=4, callbacks=[earlystop, checkpoint_1, learning_rate_reduction])\n",
        "\n",
        "# Similarly, train the other models (model2, model3, model4), and save their weights\n",
        "history2 = model2.fit(train_data, validation_data=val_data, epochs=4, callbacks=[earlystop, checkpoint_1, learning_rate_reduction])\n",
        "history3 = model3.fit(train_data, validation_data=val_data, epochs=4, callbacks=[earlystop, checkpoint_1, learning_rate_reduction])\n",
        "history4 = model4.fit(train_data, validation_data=val_data, epochs=4, callbacks=[earlystop, checkpoint_1, learning_rate_reduction])\n",
        "\n",
        "# Save model weights after training\n",
        "model1.save_weights('./model1_weights.h5')\n",
        "model2.save_weights('./model2_weights.h5')\n",
        "model3.save_weights('./model3_weights.h5')\n",
        "model4.save_weights('./model4_weights.h5')\n",
        "\n",
        "# Evaluate the models on the test data\n",
        "model1.evaluate(test_data)\n",
        "model2.evaluate(test_data)\n",
        "model3.evaluate(test_data)\n",
        "model4.evaluate(test_data)\n",
        "\n",
        "# Confusion Matrix for evaluation\n",
        "predictions = model1.predict(test_data)\n",
        "conf_mat = confusion_matrix(test_data.labels, predictions)\n",
        "print(conf_mat)\n",
        "\n"
      ],
      "metadata": {
        "id": "lQpmfKFVNA7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b696c0cc-2462-417b-f6ae-2777f05906c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/10 DMC/X-Ray/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e57ae22f8de7>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/10 DMC/X-Ray/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBb_j7SVM4WI"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained models (do not retrain)\n",
        "model1 = Create_model()\n",
        "model2 = Create_model_2()\n",
        "model3 = Create_model_3()\n",
        "model4 = Create_model_4()\n",
        "\n",
        "# Load saved weights\n",
        "model1.load_weights('./model1_weights.h5')\n",
        "model2.load_weights('./model2_weights.h5')\n",
        "model3.load_weights('./model3_weights.h5')\n",
        "model4.load_weights('./model4_weights.h5')\n",
        "\n",
        "# Function to preprocess a single image\n",
        "def preprocess_image(img_path, target_size=(224, 224)):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = img_array / 255.0  # Normalize the image\n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions on a new image\n",
        "def predict_image(model, img_path):\n",
        "    img_array = preprocess_image(img_path)\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = np.argmax(prediction)  # Get the class with the highest probability\n",
        "    return predicted_class\n",
        "\n",
        "# Test the models on new images\n",
        "img_path = 'path_to_your_image.jpg'  # Change this to the path of your image\n",
        "\n",
        "pred1 = predict_image(model1, img_path)\n",
        "pred2 = predict_image(model2, img_path)\n",
        "pred3 = predict_image(model3, img_path)\n",
        "pred4 = predict_image(model4, img_path)\n",
        "\n",
        "# Print out predictions\n",
        "print(f\"Model 1 Prediction: {pred1}\")\n",
        "print(f\"Model 2 Prediction: {pred2}\")\n",
        "print(f\"Model 3 Prediction: {pred3}\")\n",
        "print(f\"Model 4 Prediction: {pred4}\")\n",
        "\n",
        "# Accuracy: Assuming you have ground truth (true class) for your image\n",
        "true_class = 1  # Replace with actual class of your image (0: COVID, 1: NORMAL, 2: PNEUMONIA)\n",
        "accuracy1 = (pred1 == true_class)\n",
        "accuracy2 = (pred2 == true_class)\n",
        "accuracy3 = (pred3 == true_class)\n",
        "accuracy4 = (pred4 == true_class)\n",
        "\n",
        "print(f\"Accuracy for Model 1: {accuracy1}\")\n",
        "print(f\"Accuracy for Model 2: {accuracy2}\")\n",
        "print(f\"Accuracy for Model 3: {accuracy3}\")\n",
        "print(f\"Accuracy for Model 4: {accuracy4}\")\n"
      ]
    }
  ]
}